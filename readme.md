# Brewery Data Pipeline ğŸº

Este projeto implementa um pipeline de dados usando **Apache Spark** para processamento, orquestrado com **Apache Airflow**, e gerenciado em containers via **Docker Compose**. A fonte de dados Ã© a [Open Brewery DB API](https://www.openbrewerydb.org/).

---

## ğŸ”§ Requisitos MÃ­nimos

Para rodar o projeto, vocÃª precisa ter:

- Docker (>= 20.10)
- Docker Compose (>= 1.29)
- 4 GB de RAM disponÃ­veis
- Internet ativa (para chamadas Ã  API)

---

## ğŸ“ Estrutura de DiretÃ³rios

```bash
.
â”œâ”€â”€ dags/                       # DAG do Airflow
â”œâ”€â”€ data_lake/
â”‚   â”œâ”€â”€ bronze/                 # Dados brutos extraÃ­dos
â”‚   â”œâ”€â”€ silver/                 # Dados transformados
â”‚   â””â”€â”€ gold/                   # Dados agregados
â”œâ”€â”€ scripts/                    # Scripts PySpark de ETL
â”œâ”€â”€ logs/                       # Logs do Airflow
â”œâ”€â”€ plugins/                   # Plugins customizados (opcional)
â”œâ”€â”€ requirements.txt           # DependÃªncias do projeto
â”œâ”€â”€ .devcontainer/Dockerfile   # Imagem base customizada
â””â”€â”€ docker-compose.yml         # Arquitetura dos serviÃ§os

Como Rodar o Projeto
âœ… OpÃ§Ã£o 1 â€“ Rodar o pipeline manualmente (sem Airflow)
Execute cada etapa isoladamente usando:

# no bash
docker compose build -d
#Isso iniciarÃ¡: estrutura de container do docker para realizaÃ§Ã£o das tarefas
docker compose run spark-runner-extract
docker compose run spark-runner-transform
docker compose run spark-runner-aggregate
# Os resultados serÃ£o salvos localmente nas pastas data_lake/bronze, silver e gold.

OpÃ§Ã£o 2 â€“ Subir o Airflow e orquestrar tudo via DAG
1. Inicie os serviÃ§os
#bash
docker compose up --build -d
#Isso iniciarÃ¡: PostgreSQL, Redis, Airflow Webserver, Scheduler e Worker.

2. Acesse a interface do Airflow
Abra o navegador e acesse:
ğŸ‘‰ http://localhost:8080

Credenciais:

UsuÃ¡rio: admin
Senha: admin

3. Rode a DAG
Localize a DAG etl_brewery.

Clique em "Trigger DAG" (botÃ£o de â–¶ï¸).

Acompanhe os logs clicando nas tarefas: extract, transform, aggregate.

ğŸ§ª Scripts e Componentes
Todos estÃ£o em scripts/:

extract_breweries.py: coleta dados da API e salva em bronze/

transform_breweries.py: limpa, seleciona e salva dados em silver/

aggregate_breweries.py: cria agregaÃ§Ãµes por estado e cidade e salva em gold/

ğŸ“‚ Airflow DAG
Arquivo: dags/brewery_pipeline.py
Executa os 3 scripts na ordem correta com BashOperator.

ğŸ“¦ DependÃªncias Instaladas
As principais dependÃªncias estÃ£o no requirements.txt:

apache-airflow[celery,postgres]==2.7.3
pyspark
psycopg2-binary
requests
flask-session

O ambiente Ã© baseado em python:3.9 e openjdk para suporte ao Spark.

ğŸ§ª Testes e Tratamento de Erros (Em construÃ§Ã£o)
Scripts incluem verificaÃ§Ãµes de status da API, existÃªncia de diretÃ³rios e arquivos.

Logs sÃ£o registrados automaticamente via Airflow.

DiretÃ³rio tests/ contÃ©m scripts com testes simples de verificaÃ§Ã£o das funÃ§Ãµes principais.

Para rodar testes (dentro de um container com pytest instalado):

docker compose run airflow-worker pytest
ğŸ”” Monitoramento e Alertas (visÃ£o futura)
Para um ambiente de produÃ§Ã£o, recomenda-se configurar:

Alertas por e-mail via Airflow em falhas de DAG.

VerificaÃ§Ãµes de qualidade de dados antes de salvar arquivos.

Ferramentas externas como Prometheus + Grafana para monitorar tempo de execuÃ§Ã£o e falhas.

ğŸ“ ContribuiÃ§Ãµes
Sinta-se livre para abrir issues, sugerir melhorias ou criar pull requests. Ã‰ um projeto aberto para aprendizados e melhorias contÃ­nuas.

ğŸ› ï¸ Dicas e Comandos Ãšteis
bash
Copy
# Acompanhar os containers em tempo real
docker compose logs -f

# Subir Airflow manualmente com worker, scheduler e webserver
docker compose up airflow-webserver airflow-scheduler airflow-worker

# Remover containers Ã³rfÃ£os
docker compose down --remove-orphans

# Acessar um container para debug
docker exec -it <nome-do-container> bash